---
title: "p8105_hw2_msk2265"
output: github_document
---

```{r setup}
library(tidyverse)
library(readxl)
```

## Problem 2

**Read and clean the Mr. Trash Wheel sheet:**

-  **specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel**
-   **use reasonable variable names -omit rows that do not include dumpster-specific data (not sure what data is not dumpstir specific)**

**The data include a column for the (approximate) number of homes powered. This calculation is described in the Homes powered note, but not applied to every row in the dataset. Update the data to include a new homes_powered variable based on this calculation.**

```{r}
#decided to load the readxl library in case there are functions used in the background other than jusr readxl

mr_trash_wheel_df = read_excel("./data/202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N586") %>% 
  janitor::clean_names() %>% 
  mutate( 
    homes_powered = (weight_tons*500)/30 #formula based on the note in the excel spreadsheet: Homes Powered - Each ton of trash equates to on average 500 kilowatts of electricity.  An average household will use 30 kilowatts per day.
  )
```

**Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda. combine these with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to all datasets before combining.**

```{r}
#loading professor trash wheel data in the same way that Mr. trash wheel was loaded

prof_trash_wheel_df = read_excel("./data/202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M108") %>% 
  janitor::clean_names() %>% 
  mutate( 
    homes_powered = (weight_tons*500)/30 #formula same as it was for Mr. trash wheel
  ) %>% 
  mutate(
    trash_wheel = "professor"
  )#adding new column with the name of the trash wheel 

#loading gwynnda trash wheel data in the same way that Mr. trash wheel was loaded

gwynnda_trash_wheel_df = read_excel("./data/202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L157") %>% 
  janitor::clean_names() %>% 
  mutate( 
    homes_powered = (weight_tons*500)/30 #formula same as it was for Mr. trash wheel
  ) %>% 
  mutate(
    trash_wheel = "gwynnda"
  )  #adding new column with the name of the trash wheel 
  


mr_trash_wheel_df=
  mutate(mr_trash_wheel_df, trash_wheel = "mr") %>% #mutating the current Mr. trash wheel dataframe. this could be done when loading the date, but I decided to do it here because the first part of this problem does not involve adding this variable.
  mutate(year = as.numeric(year)) #had to also change to numeric since the other datasets were stored as numeric, so they could only merge if they were the same type of variable

#merging data into one

trash_wheels_df = 
  bind_rows(mr_trash_wheel_df, prof_trash_wheel_df, gwynnda_trash_wheel_df)

#creating some summaries and filters to extract summary statistics for paragraph

trash_wheels_weight = trash_wheels_df %>% 
  summarise(mean = mean(weight_tons), max = max(weight_tons), min = min(weight_tons)) #finding min max and average for weight
trash_wheels_weight

 filter(trash_wheels_df, weight_tons == trash_wheels_weight$max) #calling all of the information on the dumpster for that max weight
 filter(trash_wheels_df, weight_tons == trash_wheels_weight$min) #calling all of the information on the dumpster for that min
 
trash_wheels_homes = trash_wheels_df %>% 
  summarise(mean = mean(homes_powered), max = max(homes_powered), min = min(homes_powered)) #finding min max and average for homes powered
trash_wheels_homes
filter(trash_wheels_df, homes_powered == trash_wheels_homes$max)#calling all of the information on the dumpster for that max
filter(trash_wheels_df, homes_powered == trash_wheels_homes$max)#calling all of the information on the dumpster for that min
```

**Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables.**

For the the trash wheel dataset, there are a total of `r count(trash_wheels_df)` observations for Mr. trash wheel, professor trash wheel, and gwynnda trash wheel all together. Mr. trash wheel consisted of `r count(mr_trash_wheel_df)` observations, professor trash wheel consisted of `r count(prof_trash_wheel_df)` observations, and gwynnda trash wheel consisted of `r count(gwynnda_trash_wheel_df)`observations. The largest dumpster collection in total was 5.62 which was from mr. trash wheel, dumpster 62, the smallest was 0.61 which was from professor trash wheel, dumpster 67, and the average for all three was 3.01. The most homes powered was 93.7 again from mr trash wheel, dumpster 62, the smallest was 10.2 again from professor trash wheel, dumpster 67, and the average was 50.2. 

**For available data, what was the total weight of trash collected by Professor Trash Wheel?**

The total weight of trash collected by Professor Trash wheel is:
`r sum(pull(prof_trash_wheel_df, weight_tons))` tons.

**What was the total number of cigarette butts collected by Gwynnda in July of 2021?**

The total number of cigarette butts collected by Gwynnda in July of 2021 is:
`r sum(pull(filter(gwynnda_trash_wheel_df, year == 2021 & month == "July"), cigarette_butts))`

## Problem 3

*Background:*

*This problem uses data collected in an observational study to understand the trajectory of Alzheimer's disease (AD) biomarkers. Study participants were free of Mild Cognitive Impairment (MCI), a stage between the expected cognitive decline of normal aging and the more serious decline of dementia, at the study baseline.*

*Basic demographic information were measured at the study baseline. The study monitored the development of MCI and recorded the age of MCI onset during the follow-up period, with the last visit marking the end of follow-up. APOE4 is a variant of the apolipoprotein E gene, significantly associated with a higher risk of developing Alzheimer's disease. The amyloid Î² 42/40 ratio holds significant promise for diagnosing and predicting disease outcomes. This ratio undergoes changes over time and has been linked to the manifestation of clinical symptoms of Alzheimer's disease.*

**Instructions:**

**Import, clean, and tidy the dataset of baseline demographics. Ensure that sex and APOE4 carrier status are appropriate encoded (i.e. not numeric), and remove any participants who do not meet the stated inclusion criteria (i.e. no MCI at baseline).**

```{r}
#importing mci baseline data

mci_baseline_df = read_csv("./data/MCI_baseline.csv", skip = 1, na = ".") %>% #skipping the first line because it just contains descriptions of the variables. column titles are in row 2 of the csv. specifying that Na is "." 
  janitor::clean_names() %>% 
  mutate(
    sex = case_match(
      sex, 
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carrier",
      0 ~ "non-carrier"
    )
  ) #not dropping NA yet in order to answer the first few questions
```


```{r}
count(mci_baseline_df)
mean(pull(mci_baseline_df, current_age))

```


```{r}
#dropping the na to finish cleaning
mci_baseline_df= drop_na(mci_baseline_df, age_at_onset)
```

**Discuss important steps in the import process and relevant features of the dataset.**

For the import process, I had to skip the first row on the csv because it contained information on the variables and the actual variables were stored in the second column. I also specified the na= "." so that the . would be converted to an empty value. Then I could use drop_na to remove those rows. The data set includes the subject ID, current age, sex, education, apoe4 carrier or not, and the age of onset. 

**How many participants were recruited, and of these how many develop MCI?**
```{r}
count(mci_baseline_df)
mean(pull(mci_baseline_df, current_age))
```

Out of 483 recruted participants, 97 developed MCI

**What is the average baseline age?**

The average baseline age is 65.05 for all of the recruted participants and it is 65.61 for the participants that developed MCI.

**What proportion of women in the study are APOE4 carriers?**

```{r}
women_carriers_mci_baseline = mci_baseline_df %>% 
  filter(sex == "female") %>% 
  count(apoe4)
women_carriers_mci_baseline
```

the proportion of women who are apoe4 carriers is 30:16 or 30/46 participants who developed MCI. I used the MCI_baseline data after all of the subjects who did not developme MCI were removed.

**Similarly, import, clean, and tidy the dataset of longitudinally observed biomarker values; comment on the steps on the import process and the features of the dataset.**

```{r}
#loading the mci amyloid dataset
mci_amyloid_df = read_csv("./data/mci_amyloid.csv", skip = 1, na = "NA", col_types = "nnnnnn") %>% #skipping the first line because it just contains descriptions of the variables. column titles are in row 2 of the csv. specifying that Na is "NA" 
  janitor::clean_names() 
```

- for the mci amyloid dataset, I again had to skip the first row since the second row is what actually had the column titles. This time, the Na variables were stored as "NA" so I also specified that during the import. Lastly, I noticed that the time stamps were stored as character variables by using the view() function in my console, so i decided to specify the type of variables for each column. 

**Check whether some participants appear in only the baseline or amyloid datasets, and comment on your findings.**
```{r}
count(mci_amyloid_df)
count(mci_baseline_df)
```
since the baseline contains 97 participants and the amyloid dataset contains 487, and each line of data refers to one participant, there are clearly participants listed in the amyloid set that are not in the baseline set. To determine which participants are missing in each set, I can use an anti join, but I'm not sure that that is needed since we want the participants in both in the next section. 

**Combine the demographic and biomarker datasets so that only participants who appear in both datasets are retained, and briefly describe the resulting dataset; export the result as a CSV to your data directory.**

```{r}
mci_amyloid_df = mci_amyloid_df %>% 
  mutate(id = study_id) %>% 
  select(-study_id) #had to change id name so that they could be merged

mci_df = inner_join(mci_baseline_df, mci_amyloid_df, by = "id") #joining data

#export as csv
write_csv(mci_df, "./data/mci_total_data.csv")
```


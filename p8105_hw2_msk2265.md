p8105_hw2_msk2265
================

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

## Problem 2

**Read and clean the Mr. Trash Wheel sheet:**

- **specify the sheet in the Excel file and to omit non-data entries
  (rows with notes / figures; columns containing notes) using arguments
  in read_excel**
- **use reasonable variable names -omit rows that do not include
  dumpster-specific data (not sure what data is not dumpstir specific)**

**The data include a column for the (approximate) number of homes
powered. This calculation is described in the Homes powered note, but
not applied to every row in the dataset. Update the data to include a
new homes_powered variable based on this calculation.**

``` r
#decided to load the readxl library in case there are functions used in the background other than jusr readxl

mr_trash_wheel_df = read_excel("./data/202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N586") %>% 
  janitor::clean_names() %>% 
  mutate( 
    homes_powered = (weight_tons*500)/30 #formula based on the note in the excel spreadsheet: Homes Powered - Each ton of trash equates to on average 500 kilowatts of electricity.  An average household will use 30 kilowatts per day.
  )
```

**Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda. combine these with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to all
datasets before combining.**

``` r
#loading professor trash wheel data in the same way that Mr. trash wheel was loaded

prof_trash_wheel_df = read_excel("./data/202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M108") %>% 
  janitor::clean_names() %>% 
  mutate( 
    homes_powered = (weight_tons*500)/30 #formula same as it was for Mr. trash wheel
  ) %>% 
  mutate(
    trash_wheel = "professor"
  )#adding new column with the name of the trash wheel 

#loading gwynnda trash wheel data in the same way that Mr. trash wheel was loaded

gwynnda_trash_wheel_df = read_excel("./data/202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L157") %>% 
  janitor::clean_names() %>% 
  mutate( 
    homes_powered = (weight_tons*500)/30 #formula same as it was for Mr. trash wheel
  ) %>% 
  mutate(
    trash_wheel = "gwynnda"
  )  #adding new column with the name of the trash wheel 
  


mr_trash_wheel_df=
  mutate(mr_trash_wheel_df, trash_wheel = "mr") %>% #mutating the current Mr. trash wheel dataframe. this could be done when loading the date, but I decided to do it here because the first part of this problem does not involve adding this variable.
  mutate(year = as.numeric(year)) #had to also change to numeric since the other datasets were stored as numeric, so they could only merge if they were the same type of variable

#merging data into one

trash_wheels_df = 
  bind_rows(mr_trash_wheel_df, prof_trash_wheel_df, gwynnda_trash_wheel_df)

#creating some summaries and filters to extract summary statistics for paragraph

trash_wheels_weight = trash_wheels_df %>% 
  summarise(mean = mean(weight_tons), max = max(weight_tons), min = min(weight_tons)) #finding min max and average for weight
trash_wheels_weight
```

    ## # A tibble: 1 × 3
    ##    mean   max   min
    ##   <dbl> <dbl> <dbl>
    ## 1  3.01  5.62  0.61

``` r
 filter(trash_wheels_df, weight_tons == trash_wheels_weight$max) #calling all of the information on the dumpster for that max weight
```

    ## # A tibble: 1 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1       62 May    2015 2015-05-17 00:00:00        5.62                 15
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
 filter(trash_wheels_df, weight_tons == trash_wheels_weight$min) #calling all of the information on the dumpster for that min
```

    ## # A tibble: 1 × 15
    ##   dumpster month   year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ## 1       67 August  2020 2020-08-03 00:00:00        0.61                 12
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
trash_wheels_homes = trash_wheels_df %>% 
  summarise(mean = mean(homes_powered), max = max(homes_powered), min = min(homes_powered)) #finding min max and average for homes powered
trash_wheels_homes
```

    ## # A tibble: 1 × 3
    ##    mean   max   min
    ##   <dbl> <dbl> <dbl>
    ## 1  50.2  93.7  10.2

``` r
filter(trash_wheels_df, homes_powered == trash_wheels_homes$max)#calling all of the information on the dumpster for that max
```

    ## # A tibble: 1 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1       62 May    2015 2015-05-17 00:00:00        5.62                 15
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
filter(trash_wheels_df, homes_powered == trash_wheels_homes$max)#calling all of the information on the dumpster for that min
```

    ## # A tibble: 1 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1       62 May    2015 2015-05-17 00:00:00        5.62                 15
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

**Write a paragraph about these data; you are encouraged to use inline
R. Be sure to note the number of observations in the resulting dataset,
and give examples of key variables.**

For the the trash wheel dataset, there are a total of 845 observations
for Mr. trash wheel, professor trash wheel, and gwynnda trash wheel all
together. Mr. trash wheel consisted of 584 observations, professor trash
wheel consisted of 106 observations, and gwynnda trash wheel consisted
of 155observations. The largest dumpster collection in total was 5.62
which was from mr. trash wheel, dumpster 62, the smallest was 0.61 which
was from professor trash wheel, dumpster 67, and the average for all
three was 3.01. The most homes powered was 93.7 again from mr trash
wheel, dumpster 62, the smallest was 10.2 again from professor trash
wheel, dumpster 67, and the average was 50.2.

**For available data, what was the total weight of trash collected by
Professor Trash Wheel?**

The total weight of trash collected by Professor Trash wheel is: 216.26
tons.

**What was the total number of cigarette butts collected by Gwynnda in
July of 2021?**

The total number of cigarette butts collected by Gwynnda in July of 2021
is: 1.63^{4}

## Problem 3

*Background:*

*This problem uses data collected in an observational study to
understand the trajectory of Alzheimer’s disease (AD) biomarkers. Study
participants were free of Mild Cognitive Impairment (MCI), a stage
between the expected cognitive decline of normal aging and the more
serious decline of dementia, at the study baseline.*

*Basic demographic information were measured at the study baseline. The
study monitored the development of MCI and recorded the age of MCI onset
during the follow-up period, with the last visit marking the end of
follow-up. APOE4 is a variant of the apolipoprotein E gene,
significantly associated with a higher risk of developing Alzheimer’s
disease. The amyloid β 42/40 ratio holds significant promise for
diagnosing and predicting disease outcomes. This ratio undergoes changes
over time and has been linked to the manifestation of clinical symptoms
of Alzheimer’s disease.*

**Instructions:**

**Import, clean, and tidy the dataset of baseline demographics. Ensure
that sex and APOE4 carrier status are appropriate encoded (i.e. not
numeric), and remove any participants who do not meet the stated
inclusion criteria (i.e. no MCI at baseline).**

``` r
#importing mci baseline data

mci_baseline_df = read_csv("./data/MCI_baseline.csv", skip = 1, na = ".") %>% #skipping the first line because it just contains descriptions of the variables. column titles are in row 2 of the csv. specifying that Na is "." 
  janitor::clean_names() %>% 
  filter(id != 73 & 495) %>% #got these two id numbers from the mci_amyloid set. steps are shown when amyloid set is imported
  mutate(
    sex = case_match(
      sex, 
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carrier",
      0 ~ "non-carrier"
    )
  ) 
```

    ## Rows: 483 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (6): ID, Current Age, Sex, Education, apoe4, Age at onset
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**Discuss important steps in the import process and relevant features of
the dataset.**

For the import process, I had to skip the first row on the csv because
it contained information on the variables and the actual variables were
stored in the second column. I also specified the na= “.” so that the .
would be converted to an empty value. Finally, I looked at the
mci_amyloid data set to see which subjects did not have baseline data.
since it was only two, I was able to remove them easily by just
filtering out those two. The data set includes the subject ID, current
age, sex, education, apoe4 carrier or not, and the age of onset.

**How many participants were recruited, and of these how many develop
MCI?**

``` r
count(mci_baseline_df)
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   482

``` r
filter(mci_baseline_df, age_at_onset > 0)
```

    ## # A tibble: 97 × 6
    ##       id current_age sex    education apoe4       age_at_onset
    ##    <dbl>       <dbl> <chr>      <dbl> <chr>              <dbl>
    ##  1     3        62.5 male          16 carrier             66.8
    ##  2     5        66   male          16 non-carrier         68.7
    ##  3     7        66.5 male          18 non-carrier         74  
    ##  4    13        63.1 male          12 carrier             69  
    ##  5    14        58.4 female        20 non-carrier         66.2
    ##  6    18        67.8 male          16 non-carrier         69.8
    ##  7    22        67.3 female        20 carrier             74.6
    ##  8    26        64.8 female        20 carrier             71.1
    ##  9    30        66.3 female        12 non-carrier         73.1
    ## 10    39        68.3 female        16 carrier             70.2
    ## # ℹ 87 more rows

Out of 483 recruited participants, 97 developed MCI

**What is the average baseline age?**

``` r
mean(pull(mci_baseline_df, current_age))
```

    ## [1] 65.04315

The average baseline age is 65.04.

**What proportion of women in the study are APOE4 carriers?**

``` r
#calculating the proportion of women who are carriers
women_carriers_mci_baseline = mci_baseline_df %>% 
  filter(sex == "female") %>% 
  count(apoe4)
women_carriers_mci_baseline
```

    ## # A tibble: 2 × 2
    ##   apoe4           n
    ##   <chr>       <int>
    ## 1 carrier        63
    ## 2 non-carrier   147

``` r
147+63
```

    ## [1] 210

the proportion of women who are apoe4 carriers is 63:147 or 63/210
participants who developed MCI.

**Similarly, import, clean, and tidy the dataset of longitudinally
observed biomarker values; comment on the steps on the import process
and the features of the dataset.**

``` r
#loading the mci amyloid dataset
mci_amyloid_df = read_csv("./data/mci_amyloid.csv", skip = 1, na = "NA", col_types = "nnnnnn") %>% #skipping the first line because it just contains descriptions of the variables. column titles are in row 2 of the csv. specifying that Na is "NA" 
  janitor::clean_names() 
```

    ## Warning: One or more parsing issues, call `problems()` on your data frame for details,
    ## e.g.:
    ##   dat <- vroom(...)
    ##   problems(dat)

- for the mci amyloid dataset, I again had to skip the first row since
  the second row is what actually had the column titles. This time, the
  Na variables were stored as “NA” so I also specified that during the
  import. Lastly, I noticed that the time stamps were stored as
  character variables by using the view() function in my console, so i
  decided to specify the type of variables for each column.

**Check whether some participants appear in only the baseline or amyloid
datasets, and comment on your findings.**

``` r
count(mci_amyloid_df)
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   487

``` r
count(mci_baseline_df)
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   482

since the baseline contains 482 participants and the amyloid dataset
contains 487, and each line of data refers to one participant, there are
clearly participants listed in the amyloid set that are not in the
baseline set. To determine which participants are missing in each set, I
can use an anti join, but I’m not sure that that is needed since we want
the participants in both in the next section.

**Combine the demographic and biomarker datasets so that only
participants who appear in both datasets are retained, and briefly
describe the resulting dataset; export the result as a CSV to your data
directory.**

``` r
mci_amyloid_df = mci_amyloid_df %>% 
  mutate(id = study_id) %>% 
  select(-study_id) #had to change id name so that they could be merged

mci_df = inner_join(mci_baseline_df, mci_amyloid_df, by = "id") #joining data

#export as csv
write_csv(mci_df, "./data/mci_total_data.csv")

#looking at the mean of those who developed MCI
mean(
  pull(
    filter(
      mci_df, age_at_onset>0),
    time_8
    ),
  na.rm=TRUE
  )
```

    ## [1] 0.1061211

``` r
mean(
  pull(
    filter(
      mci_df, age_at_onset>0),
    baseline
    ),
  na.rm=TRUE
  )
```

    ## [1] 0.1095434

The resulting dataset contains 483 participants, their sex, baseline
age, education, age at onset, carrier status, and 4 additional time
points of mci data. Not all participants have all 4 time points of data.
The average mci at baseline is 0.1109522 and at time 8 is 0.1082453,
which shows a slight decrease. The mean for those who had MCI was 0.1095
at baseline and 0.106 by time 8.
